#### day10_파이썬10(200102)



##### 스크래핑

* Beautifulsoup 사용해서 웹스크래핑 하게 됨

  

* beautifulsoup 객체 생성, 첫번째 변수에는 반드시 html 저장되어있는 변수 지정해줘야, 뒤엔 분석기 종류 지정해줘야 함(html.parser) => soup이라는 변수에 담아줌

  ==> 'html 문서 분석(파악)'으로 봐야함 , soup에 분석결과로서 원문이 그대로 출력

  ==> 원하는 태그 추출할 수 있어야

  == > soup.태그이름(html/body/h1/p)로 원하는 부분 추출 가능

```python
from bs4 import BeautifulSoup

# html 파일 가져왔다는 가정하에 실습
html = """
<html><body>
<h1>스크래핑</h1>
<p>웹 페이지 분석</p>
<p>원하는 부분 추출</p>
</body></html>
"""

# html 문서에서 원하는 부분 추출
soup=BeautifulSoup(html,'html.parser') 


soup.html.body.h1 # <h1>스크래핑</h1>
soup.html.body.p 
# <p>웹 페이지 분석</p> => 가장 처음으로 만나는 p태그 추출, 두번째는 참조 X
```

* 첫 번째 p 아닌 두번째 p 추출하기 위한 방법 
  * 형제 태그(=수준,레벨 같아 같은 body 태그로 감싸져 있는) 추출하기
        - p1 => 엔터문자 => 형제태그

```python
p1=soup.html.body.p
print(p1.next_sibling) # \n
# p1.next_sibling => 문자열 뒤에 있는 엔터문자(\n)를 참조하게 됨, 한 번 더 해줘야 두번째 태그 참조 가능
p2=p1.next_sibling.next_sibling # <p>원하는 부분 추출</p>

# 태그 없이 문자만 추출하는 법 => string 붙여주면 됨
# h1.string
p1.string  # '웹 페이지 분석'
p2.string  # '원하는 부분 추출'
```

* find 함수
  * id를 이용하여 원하는 데이터에 직접 접근하여 추출하기
  * id 사용하기 위해 html에 id 추가해줘야 함

```python
html = """
<html><body>
<h1 id="title">스크래핑</h1>
<p id="body">웹 페이지 분석</p>
<p>원하는 부분 추출</p>
</body></html>
"""

# id 사용하기 위해 html에 id 추가해줘야 함
soup=BeautifulSoup(html,'html.parser') 
title=soup.find(id="title")  # <h1 id="title">스크래핑</h1>
print("title="+title.string) # title=스크래핑
body=soup.find(id="body") # <p id="body">웹 페이지 분석</p>
print("body="+body.string) # body=웹 페이지 분석
```

* find_all()
  *  여러 개의 태그를 한번에 추출하고자 할 때 사용하는 함수

```python
html="""
<html><body>
<ul>
<li><a href="http://www.naver.com">naver</a></li>
<li><a href="http://www.daum.net">daum</a></li>
</ul>
</body></html>
"""

soup=BeautifulSoup(html,'html.parser')
links=soup.find_all("a") # 특정태그 부분 한꺼번에 추출하기
[<a href="http://www.naver.com">naver</a>,
<a href="http://www.daum.net">daum</a>]

for a in links:
    if 'href' in attrs: 
    # a태그에 href라는 attribute(속성) 있는지 조사하는 것
    href=a.attrs['href'] 
    # attrs 사용해서 href 속성에 대한 값 추출 
    # -> attrs 자주 사용, 기억 잘 해둬야 #
    text=a.string # 문자열만 출력
    print(href) 
    # => http://www.naver.com 추출 => 밑에 주소 누르면 네이버로 이동
    print(text,"->", href)
    ==> naver -> http://www.naver.com
	    daum -> http://www.daum.net
```



###### 기상데이터 가져오기

* urlopen, beautifulsoup 사용
* 기상예보에서 특정 내용 추출
  - http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp

```python
import urllib.request as req

url="http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp"
res=req.urlopen(url) 
# <http.client.HTTPResponse at 0x14fdc0710c8>: 객체로 담김
soup=BeautifulSoup(res,'html.parser') 
# res객체를 html로 분석하여 넣어줌


# soup에 저장된 내용 중 특정텍스트 <title>기상청 육상 중기예보</title> 검색
# 1
soup.title.string 
# => '기상청 육상 중기예보'

# 2
title=soup.find('title').string
# => 기상청 육상 중기예보


# soup에 저장된 내용 wf 태그로 감싸져있는 부분 추출하기
wf=soup.find("wf").string
print(wf)
```



###### CSS

:  HTML에 스타일 지정, 적용해서 웹 만드는 것

- CSS 선택자(크롬 => 개발자도구 => 객체선택 => copy selector) 사용
    - soup.select_one(선택자): 선택자로 지정된 요소 하나를 추출
    
    - soup.select(선택자): 선택자로 지정된 여러 개 요소를 추출
    
      - 태그가 여러개인 경우 가정한다면 id를 지정해줘서 추출
        - id는 #으로 지정
        - 태그 지정할 때는 띄어쓰기 or >(꺽새)로 표시
        - class는 .로 지정

```python
html="""
<html>
    <body>
        <div id="myid">
            <h1>2020년</h1>
            <ul class='day'>
                <li>월요일</li>
                <li>화요일</li>
                <li>수요일</li>
            </ul>
        </div>
    </body>
</html>
"""

# css 선택자로 내용 추출하기
soup=BeautifulSoup(html,'html.parser')

# h1 추출하기
print(soup.select_one("h1").string) 
# 2020년: 원래는 None이 나오는데 h1 태그가 하나라 나올 수 있었던 것
print(soup.select_one("div#myid h1").string) # 2020년
print(soup.select_one("div#myid > h1").string) # 2020년
# div#myid: div 태그가 여러개인 경우라면 #로 id 지정
# 태그 지정할 때는 띄어쓰기 or >(꺽새)로 표시


# 요일 추출하기
#class는 .로 지정
print(soup.select_one("div#myid ul.day li")) 
# => slelect_one은 하나만 추출 => <li>월요일</li>
myList=soup.select("div#myid ul.day li")
for a in myList:
    print(a.string)
```



###### finance.yahoo.com 에서 삼성 주가 추출

```python
from bs4 import BeautifulSoup
import urllib.request as req

url="https://finance.yahoo.com/quote/005930.KS?p=005930.KS&.tsrc=fin-srch&guccounter=1&guce_referrer=aHR0cHM6Ly9maW5hbmNlLnlhaG9vLmNvbS8&guce_referrer_sig=AQAAAAV5iTDGhBWuXXCRm1v9jSTkhKAGtjMeKms9_QA1vkP3oxTxXgGCMnF8TUgSwunh3PN9RZ257Zs0hhudbRoM6vsCtiIFfuugXlZuElpNBVg13NHEo5OkbWRWuburA3x-wOxdlIMB7tU3Thrkjyp9884R4DXWIIHn3Inxzg3Pk5WP"
res=req.urlopen(url)
soup=BeautifulSoup(res,'html.parser')

# 한번에 접근하기 
re=soup.select_one("#quote-header-info > div.My\(6px\).Pos\(r\).smartphone_Mt\(6px\) > div > div > span.Trsdu\(0\.3s\).Fw\(b\).Fz\(36px\).Mb\(-4px\).D\(ib\)")
print(re)  # None


# 일일이 찾아서 들어가기
soup.select_one("#quote-header-info")
soup2=soup.select("#quote-header-info > div")[2] 
# div 리스트 중 2 index에 주가 포함되어 있으므로
print(soup2.select_one("div > span").string) # 59,100.00
```



###### 위키도피아에서 윤동주 시인 작품 추출하기

```python
from bs4 import BeautifulSoup
import urllib.request as req

url='https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC'
res=req.urlopen(url)
soup=BeautifulSoup(res,'html.parser')
re=soup.select("#mw-content-text > div > ul > li > ul > li > a") 
# ul:nth-child(6) => 컴퓨터에 따라 에러날 수도 있어, 뺴도 결과 똑같이 나오기 때문에 빼도 됨, 경로가 유니크하면 문제 x

for r in re:   
    print(r.string)
 

# >와 띄어쓰기 차이

# >
# li > a: 마지막 a는 li 태그내부에 있는 a 태그
a=soup.select("#mw-content-text > div > ul:nth-child(6) > li > a")
print(len(a)) # 1
==> 맨 처음 하나의 li만 나옴

# 띄어쓰기
# li a: 모든 li 태그 내에 있는 a 태그
b=soup.select("#mw-content-text > div > ul:nth-child(6) > li  a") 
print(len(b)) # 21
==> 전체 li 나옴
```



###### 여러가지 방법으로 추출하기

```python
from bs4 import BeautifulSoup
import urllib.request as req

html="""
<ul id="language">
    <li id="bas">Basic</li>
    <li id="cpp">C++</li>
    <li id="ja">Java/li>
    <li id="py">python</li>
    <li id="sp">Spark</li>
</ul>
"""


# 추출하는 방법
sel=BeautifulSoup(html,'html.parser')
print(sel.select_one("#py").string)

# python 추출하는 방법 여러가지
myFunc=lambda arg: print(sel.select_one(arg).string)
myFunc("#py")
myFunc("li#py")
myFunc("ul li#py")
myFunc("#language #py")
# myFunc("#language > #py")
# myFunc("ul#language > li#py")
myFunc("li[id='py']")
# myFunc("li:nth-of-type(4)")
print(sel.select("li"))
print(sel.select("li")[3].string)
print(sel.find_all("li")[3].string)
```



###### 파이참에서 만든 html(fru-veg)파일 가져오기
  * 파이참 파일 복사해서 소스코드 있는 폴더에 붙여넣기 한 후

```python
fp=open("fru-veg.html",encoding="utf-8")
soup=BeautifulSoup(fp,"html.parser")


# 아보카도 추출하기 = 여러가지 방법
print(soup.select_one("li:nth-of-type(6)").string) 
=> 파이썬 버전에 따라 에러나오기도. 에러
print(soup.select_one("#ve > li:nth-of-type(4)").string)
# print(soup.select("#ve > li[data-lo = 'us']"))
print(soup.select("#ve > li.black")[1].string)


# find 메서드
cond={'data-lo':'us', 'class':'black'}
print(soup.find('li', cond))
```



###### 정규식과 데이터 추출하기

```python
# 정규식과 함께 데이터 추출하기: 파이썬 라이브러리를 활용한 데이터분석 교재 300p, 요약 참고

html="""
<li><a href="test.html">test</li>
<li><a href="https://test.html">test2</li>
<li><a href="https://test.html">test3</li>
<li><a href="http://test.html">test4</li>
"""
import re
soup=BeautifulSoup(html, 'html.parser')
li=soup.find_all(href=re.compile("https://"))
print(li)
==> [<a href="https://test.html">test2</a>, <a href="https://test.html">test3</a>]
```











