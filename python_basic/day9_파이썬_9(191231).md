### day9_파이썬9(191231)



#### 스크래핑

* 데이터 수집: beautifulsoup 사용

- 데이터 수집만큼 중요한 것이 데이터전처리하는 것. 분석하기 적절한 형태로 변형해야 함. (판다스,넘파이 활용)
- 데이터 수집
    - DB(정형, formal data)에서 데이터 가져오기
    - 텍스트(비정형 데이터: 웹에 있는 데이터) 수집
        - DB에서 데이터 가져오기
        - 웹에서 크롤링, 스크래핑으로 데이터 추출하기
- 파이썬 웹 데이터 추출
    - urllib 라이브러리(=모듈집합) 사용
        - request 모듈: urllib 라이브러리 안에 들어가 있음(=> urllib.request로 접근)
        - request 모듈로 웹에 있는 데이터에 접근. urlretrieve함수를 이용하여 웹 파일을 다운로드 수행
        
        

* urlretrieve 이용한 다운로드 -> 파일로 저장됨

```python
# url과 저장경로 지정
url="https://www.multicampus.com/img/saas/main/logo/CUS0001/pc_main.png"
savename="test.png"

# 다운로드
import urllib.request # 웹에 있는 자료 내려받기 위한 모듈
urllib.request.urlretrieve(url, savename) 
# urlretrieve함수(다운받고자 하는 주소, 경로): 데이터 다운받은 후 경로에 파일로 저장해줌 => 다운로드 데이터 하드디스크에 저장한 것
print("저장되었습니다")
==> test.png이미지 저장 후 저장되었습니다 출력
```

* urlopen을 이용한 다운로드
  * urlopen 다운로드시 파일로 저장되는 것이 아니라, 데이터가 메모리에 적재
  * 컴퓨터 작업:하드디스크 -> 메모리(램) -> CPU 처리

```python
url="https://www.multicampus.com/img/saas/main/logo/CUS0001/pc_main.png"
savename="test2.png"

mem=urllib.request.urlopen(url).read()
# urllib.request.urlopen(url): 해당페이지로 이동, 다운받기 위한 준비작업 끝남
# read(): 파일을 읽어들임
# mem: 메모리(=램:cpu가 연산하기 위한 자료 적재되는 장소)에 올라간 변수, 변수는 다 메모리에 올라가 있음
print(mem)

# 메모리에서 읽어진 mem을 파일로 저장하기
with open(savename, mode="wb") as f: # wb: 이미지 파일=바이너리 파일을 쓰기모드로 열어줄 때
    f.write(mem)
    print("저장되었습니다")
```



```python
import urllib.parse

addr="http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp"
values={'stnId':'156'} 
# 자유롭게 원하는 지역 지정해서 문서 가져올 수 있도록 딕셔너리로 지정해준 것
# values={'stnId':'156','108',...}와 같이 참조하고픈 지역 많을 때 딕셔너리 사용하는 것이 편리

param=urllib.parse.urlencode(values)
addr # 'http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp'
values # {'stnId': '156'}
param # 'stnId=156'
addr+'?'+param 
==> 'http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=156'
url=addr+'?'+param
url

data=urllib.request.urlopen(url).read()
text=data.decode('utf-8')
print(text)
```



* 스크래핑
  * 원하는 정보를 추출 => beautifulsoup 활용해서 원하는 자료 긁어올 수 있음

- 네이버 파이썬 지식인 1p
  https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Aall%2Cp%3Aall&query=%ED%8C%8C%EC%9D%B4%EC%8D%AC&c_id=&c_name=&sm=tab_pge&kin_start=1

- 네이버 파이썬 지식인 2p
  https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Aall%2Cp%3Aall&query=%ED%8C%8C%EC%9D%B4%EC%8D%AC&c_id=&c_name=&sm=tab_pge&kin_start=11

- 네이버 파이썬 지식인 3p
  https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Aall%2Cp%3Aall&query=%ED%8C%8C%EC%9D%B4%EC%8D%AC&c_id=&c_name=&sm=tab_pge&kin_start=21

  

- 위의 주소를 보고 start 뒤에 숫자만 달라진다는 것 알게 됨 => 공통부분 묶으주고 숫자 변수 p로 준 후 주소 지정해서 원하는 페이지 가져올 수 있음
https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade

* 데이터 수집 과정(1~3번)
  1) 수집 대상 웹사이트에서 개발자 도구 활용하여 스크랩 대상에 대한 선택자 복사

  * 위키피디아 > 윤동주시인 > 같이보기 영역 고유 주소 => 고유 주소 알면 추출, 데이터 가져오기 가능해짐
  - mw-content-text > div > div:nth-child(77) > table > tbody > tr > td

  2) 파이썬에서 urlretrieve, urlopen 등을 사용하여 웹페이지를 가져온다
  3) 파서를 사용하여 파싱한다

   - 파싱(parsing): 가져온 웹문서에서 원하는 데이터를 추출하는 것
  - 태그를 기준으로 원하는 데이터 추출이 가능함

  4) 데이터에 대한 전처리를 수행
  5) 모델링..

```python
# 네이버 환율 : 미국 

import urllib.request as req
from bs4 import BeautifulSoup

url="https://finance.naver.com/marketindex/exchangeDetail.nhn?marketindexCd=FX_USDKRW#"
res=req.urlopen(url) 
# <http.client.HTTPResponse object at 0x000001CFF30A98C8>
soup=BeautifulSoup(res, 'html.parser') # html 쭉 나오게 됨
p=soup.select_one("#content > div.section_calculator > table:nth-child(4) > tbody > tr > td:nth-child(1)").text
print(p)
```





##### 파일 종류

* HTML
  * 비구조적 문서
* HTML로 작성된 웹문서가 대부분(주류로 사용), 그러나 HTML 문서는 가독성 많이 떨어져, 형식이 정해져있지 않고 구조가 없기 때문에.

- 컴퓨터가 HTML 문서 이해하기 힘들어 한계 많음. 해석은 가능하나 어려움.
  
  ```html
  <P>오늘의 날씨는 맑습니다.</P> 
  => 컴퓨터가 이해 X
  ```

* XML

* 구조적 문서 == 계단식으로 설계되어 있는 문서

  * 컴퓨터가 이해하기 나은 새로운 문서 규격이 필요 => XML이 만들어짐.

  * 사용자가 정의하는 태그로 구조화되는 문서로 문서 자체가 해석 가능, 용이

  * 의미있는 태그로 데이터 묶여 문서에서 내가 원하는 자료 가져오기 용이함

  * rss: 태그이름, 전체적 구조 rss 태그로 묶어줌

    ```xml
    <날씨>
          <오늘> 맑음 </오늘>
    </날씨>
    
    => 날씨 안에 오늘 안에 맑음으로 구조적으로 설계되어 있음
    ```

* JSON

  *  딕셔너리 자료 구조로 표현되어진 문서 규격으로 가벼워 전송빠르고 연산양 작아 많이 사용

    ``` json
     {날씨: {오늘: 맑음}}
    ```



##### 문자열 인코딩

* 파이썬 문자열 인코딩

* 인코딩(encoding) : 유니코드 -> 바이트 열로 변환 (문자표현 -> 2진수열(바이트열))

* 인코딩 방식 : ASCII, EUC-KR, CP949

  => 인코딩 방식을 통일하고자 유니코드 (유니버셜 -> 범용 코드) 만듦

  * 유니코딩 (UTF-8, UTF-16, UTF-32....)
  * UTF-8이 가장 일반적으로 사용됨
  * 파이썬 문자열 자료형은 UTF-8

* 디코딩(decoding) : 바이트 열 => 유니코드로 변환

```python
ext = "안녕" # 유니코드
print(len(text))

l=bytearray(text,'cp949') # 유니코드 -> 바이트
len(l) # 4

l=bytearray(text,'utf-8') 
len(l) # 6: 한글 한글자당 3바이트씩 할당
```

```python
test="안녕하세요"
type(test) # str

# 인코딩: 문자 -> 바이트열
s1=test.encode('cp949') 
# cp949로 인코딩된 결과가 저장. 바이트 변환되어서
print(type(s1)) # <class 'bytes'>
print(len(s1)) # 10
s1.decode('cp949') # '안녕하세요'
s1.decode('euc-kr') # '안녕하세요'
s1.decode('utf-8') 
=> 'utf-8' codec can't decode byte 0xbe in position 0: invalid start byte
# 인코딩, 디코딩 바이트가 맞아야 변환 가능


s2=test.encode('euc-kr')
print(type(s2)) # <class 'bytes'>
print(len(s2)) # 10


s3=test.encode('utf-8')
print(type(s3)) # <class 'bytes'>
print(len(s3))  # 15
```

