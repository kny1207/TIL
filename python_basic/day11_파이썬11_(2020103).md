#### day11_파이썬11(2020013)



###### 200102 연습문제

- 클래스는 단독으로 실행 x

- 클래스로부터 객체를 생성해줘야 함, 객체는 RAM에 만들어짐
    - 객체는 프로그램의 기본 구성요소, 프로그램이 동작할 수 있도록 중요한 기능, 동작 수행하는 주체임

- 프로그램이 실행된다는 것은 객체들이 동작하면서 인터렉션을 하는 것이라고 생각하면 됨

    ```python
    first=Point(3,4)
    def __init__(self x,y):
        self.x=x
        
        init(self, 재료배달부):
            붕어빵.재료=재료배달부
            붕어빵기계(공장에서 제조한 밀가루)
    ```

    - first=Point(3,4)  

        ==> 클래스 생성되어질수록 객체 생성하는 것, Class Point()로 가서 실행됨

    - def __init__(self x,y): 
        
         ==> 생성자 메서드 무조건 호출됨: x,y에 3과 4가 전달됨

    * self.x=x 

       ==> self.x는 객체 자신을 의미, 지금 만든 객체를 말함. 

      ​		self.x와 x는 다름! 

      ​		x는 임시변수(지역변수)로 매개체와 비슷

      ​		self.x는 현재 전달된 객체가 가지는 속성으로 x는 self.x에 속성 전달 위해		잠시 가지고 있는 임시 변수이고 self.x는 객체의 고유 속성물.

    - 객체들마다 고유속성과 값이 다르게 저장된다. init로 초기화한 후 객체마다 다른 값 전달받음

    * init(self, 재료배달부): 

      ==> 공장에서 제조한 밀가루를 재료 배달부에게 ==> 붕어빵 재료에 재료배달부가 밀가루 전달

    ```python
    # 클래스
    class 붕어빵기계(self. 내용물,반죽)
        self.내용물=내용물
        self.반죽=반죽
    # 붕1=붕어빵기계(단팥,밀가루) -> 단팥밀가루 붕어빵(=객체1)
    # 붕2=붕어빵기계(크림,밀가루) -> 크림밀가루 붕어빵(=객체2)
    # 붕3=붕어빵기계(단팥,쌀가루) -> 단팥쌀가루 붕어빵(=객체3)
    ==> 붕어빵(=객체) 3개 만들어지는 것 == self 객체 3개 만들어진 것
    
    # 메서드
    굽는다(self, 시간, 방법): 
    # self: 메서드 호출하는 당시 시점에 전달되어지는 객체 의미, 호출되어지는 시점에 객체 각각이 self가 된다. 
    # self를 this와 같은 대명사라고 생각하면 된다.
        self.시간=시간 
        self.방법=방법
        
    붕1.굽는다(1분, 타지않게)  
    # 붕1객체에 굽는다메서드(=객체수행동작) ==> 메서드의 self 붕1
    붕2.굽는다(30초, 타지않게) ==> 메서드의 self 붕2
    붕1.굽는다(2분, 타지않게) ==> 메서드의 self 붕3
    ```



##### hanbit에 로그인하고 나의 정보 가져오기

- 내부적으로 동작하는 메커니즘 이해하기 좋아
    - 로그인하고 행해지는 작업이라 내 데이터만 가져올 수 있어, 데이터 양 적어 많이 쓰이진 않음
    - 서버--클라이언트 사이에서 오고가는 데이터 확인 할 수 있음

- http://www.hanbit.co.kr/
    - 가입하면 2coin 줘서 가져오는 실습 작업 할 것
        - skdus1207 rkgh0411!

        

* 내부 동작

  http://www.hanbit.co.kr/index.html 

  1) 클라이언트: www.habit.co.kr(주소) 입력

  2) 서버: http://www.habit.co.kr/index.html 클라이언트에게 전달(신호 함께 전달)

  ==>  정상 신호:200, 주소 잘못입력:40x, 서버내부오류:50x

  3) 클: 웹브라우저가 http://www.habit.co.kr/index.html 해석 

>  결과 화면 출력: 우리가 보는 화면

* 로그인 > https://www.hanbit.co.kr/member/login.html

> m_id <- 아이디, m_passwd 
>
> => 비밀번호 전달한 후 로그인 처리, 인증작업 딕셔너리 키 값으로 넣어줘야함
> 로그인인처리(login_proc.php) 빠르게 주소창에 나타났다 사라지면서 로그인 처리해줌
>
> ![2000103_로그인처리과정확인2](images/2000103_로그인처리과정확인2.png)
>
> 로그인 클릭 안 누르고 바로 login_proc.php 파일로 id,pw 넘겨줄 것
>
> > 처리 결과: 로그인 성공, 실패

- 로그인처리 되어지는 과정 보기
    
    ![2000103_로그인처리과정확인3](images/2000103_로그인처리과정확인3.png)
    
    > 메인: F12 > 네트워크 > DOC(=문서내용) 선택 + Preserve log(=기록 보존) 선택: 내 작업에 따라 수행되는동작 확인하기 위해> 로그인 > login.html 생김(status: 200) > 로그인하면 login_proc.php + login.html 파일 실행 > login_proc.php 클릭 > Headers: post(=외부에 드러나지 x) > index.html 상태로 메인 화면 가게 됨
    >
    > > 로그인 처리는 클라이언트가 아닌 한빛서버에서 해주는 것
    
    

* 사용자 페이지(http://www.hanbit.co.kr/myhanbit/myhanbit.html)에서 마일리지, 한빛이코인 금액 출력 

```python
# 로그인 하기 위한 모듈
import requests
# html 문서로 작성된 경우, requests로 원하는 부분 추출 가능, javascript가 많은 경우엔 어려움
from bs4 import BeautifulSoup

# id,pw 입력
USER="skdus1207"
PASS="rkgh0411!"

# 세션(=서버와 클라이언트간 연결) 시작하는 코드 만들기: 나는 클라이언트로 서버에 연결하려는 작업 수행하는 것
session=requests.session() 
# 승차권: 세션 시작할 수 있도록 해주는 session 객체를 만든 것, 아직 서버 연결 x, 동작해야 연결됨. 세션을 만든 것은 승차권을 갖는 것과 비슷


# 인증작업: id,pw 딕셔너리로 만들어 주는 것이 편리
login_info={'m_id':USER,'m_passwd':PASS}

# login할 웹 주소 지정
url_login="https://www.hanbit.co.kr/member/login_proc.php" 
# 한빛 f12 > 네트워크에서 복사해옴


# 로그인 처리
res=session.post(url_login, data=login_info) 
# post: id,pw 외부에 드러나지 않도록 함: url 주소와 data 인수로 id들어있는 login_info 넘겨줌
res  # <Response [200]>  ==> 로그인 성공, 메인 페이지        
res.raise_for_status() 
==> 오류가 발생하면 예외상황을 출력해줌. url 접속, 연결하는 코드 밑에 써주는 것이 좋음. 이 코드 없으면 에러 이유 안 알려주고 실행이 x
# 문제가 있을 경우 에러 알려주는 코드
# 주소 proc9로 입력한 경우 => 404 Client Error: Not Found for url: https://www.hanbit.co.kr/member/login_proc9.php


# 사용자 페이지로 이동하는 코드 작성
res2=session.get("http://www.hanbit.co.kr/myhanbit/myhanbit.html")
res2 # <Response [200]> ==> 로그인 상태로 사용자 페이지 온 것, text 문서로 html res2에 들어가있음
res2.raise_for_status() # 에러 확인하기 위한 코드

# 마일리지,이코인 뽑아오기
soup=BeautifulSoup(res2.text,'html.parser')
# 마일리지
mileage=soup.select_one("#container > div > div.sm_mymileage > dl.mileage_section1 > dd > span").get_text()
# 이코인
ecoin=soup.select_one("#container > div > div.sm_mymileage > dl.mileage_section2 > dd > span").get_text()
print("마일리지:" + mileage)  # 마일리지:2,000
print("이코인:" + ecoin)  # 이코인:0
```



##### 네이버 시작페이지 캡쳐 

* 캡쳐 ==> 이미지(png)로 저장하기

* 웹브라우저를 이용한 스크래핑

* selenium: 웹브라우저를 조작(스크래핑) 도구

    :자동으로 url 열기, 클릭, 스크롤, 문자 입력, 화면캡쳐 해주는 자동화 도구

* selenium 설치

    1) 아나콘다 프롬프트: pip install selenium

    

* Phantomjs: 화면없이 명령줄에서 이용할 수 있는 웹브라우저

 * pantomJS 설치하기

    1)https://phantomjs.org/download.html 이동 및 다운로드

    2)path 지정하기

    ![2000103_phantomjsexe_PATH추가](images/2000103_phantomjsexe_PATH추가.png)

    * PhantomJS가 설치된 경로를 고급설정->path에 추가(또는 기존 path 경로에다가 phantomjs.exe파일을 복사)

      1. bin에 있는 phantomjs.exe 복사해서 c://windows에 붙여넣기 => 위치 상관없이 실행 가능

      2. 내 pc > 속성 > 고급 설정 > 환경변수 > path 더블 클릭 > 새로 만들기 > phantomjs.exe 경로 추가(C:\Users\student\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin) => 위치 상관없이 실행 가능(사진 첨부)

         * a폴더: abc.exe

         * b폴더에서 abc 실행하면 -> 안됨,
           => a폴더를 PATH에 추가하면 다른 경로에서도 a폴더에 있는 파일을 다른 곳에서도, 어디서든 실행 가능

      3. 크롬 드라이버 설치(카페 첨부파일에서)



* Phantomjs(=크롬으로 생각)를 이용하여 사이트 탐색(이동) 가능: 네이버로 가기
* selenium 이용하여 해당 사이트에서 자동으로 url 열기, 클릭, 스크롤, 문자 입력, 화면캡쳐 등의 작업을 수행

```python
# selenium
from selenium import webdriver
url="http://www.naver.com"

# phantomjs 드라이버 추출
browser=webdriver.PhantomJS()
# 3초 대기
browser.implicitly_wait(3) 

# url로부터 데이터 읽기
browser.get(url)

# 화면 캡쳐
browser.save_screenshot("myshot.png") # => 작업하고 있는 폴더에서 확인 가능

# 브라우저 종료
browser.quit()
```



#####  rottentomateos에서 영화랭킹 가져오기

```python
import urllib.request as req
page=req.urlopen("https://www.rottentomatoes.com/")
doc=page.read()
soup=BeautifulSoup(doc,'html.parser')
movies=soup.find(id="homepage-top-box-office")
movies_list=movies.find_all("td", class_="middle_col") 
# class_ 속성으로 클래스 속성 지정해주면 해당 클래스만 추출가능
for movie in movies_list:
    print(movie.get_text())
    print("https://rottentomatoes.com/"+movie.find("a").get("href"))
```



##### stackoverflow에서 python 질문 뽑아오기

```python
import requests

url="https://stackoverflow.com/questions/tagged/python"
response=requests.get(url)
soup=BeautifulSoup(response.text, 'html.parser')

links=[]
for link in soup.select("div.question-summary h3 a"):
    links.append(link.attrs['href'])
    print("http://stackoverflow.com"+link.attrs['href'])
for link in links:
    url="http://stackoverflow.com"+link  
    #질문 답변 들어가있는 경로 url => 답변있는 page url을 url에 담아줌
    response=requests.get(url)
    text=response.text
    soup=BeautifulSoup(text,'html.parser')
    print("Qustion\n")
    for x in soup.select("div.postcell div.post-text p"):
        print(x.text)
    print("\nAnswer\n")
    for x in soup.select("div.answercell div.post-text p"):
        print(x.text)
    print("="*40,"\n")
```



#####  네이버 파이썬 지식인 가져오기

1. 파이썬 키워드에 대한 질문 추출하기
2. 1페이지에 있는 질문 모두 추출
3. 1~10 페이지에 있는 질문 모두 추출
4. 1~10 페이지에 있는 질문/답변 모두 추출:이미지 x, text만 ==> 해당 url로 들어가야 가능
5. 추출한 전체 결과에 대해 '초보'라는 단어가 등장한 횟수 출력

```python
import requests
from bs4 import BeautifulSoup

# 1페이지 질문 제목 추출하기
url="https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Aall%2Cp%3Aall&query=%ED%8C%8C%EC%9D%B4%EC%8D%AC&c_id=&c_name=&sm=tab_pge&kin_start=1"
response=requests.get(url)
soup=BeautifulSoup(response.text, 'html.parser')
a=soup.select("#elThumbnailResultArea > li > dl > dt.question >a")
for i in a:
    print(i.text)

# 1페이지 질문 내용 추출하기
links=[]
for link in a:
    links.append(link.attrs['href'])


t=[]
a=[]
for link in links:
    url=link
    response=requests.get(url)
    text=response.text
    soup=BeautifulSoup(text,'html.parser')
#     print("\nQuestion")
#     for x in soup.select("div.question-content div.c-heading._questionContentsArea.c-heading--default-old div.c-heading__title > div"):
#         print(x.text)

#     print("\nAnswer\n")
    for x in soup.select("div.question-content div.c-heading__content"):
        t.append(x.text)
#     print("="*100,"\n")

# 답변 내용 추출하기
    for x in soup.select("#SE-57d00d89-6236-4292-b796-ce13aca64af6 > div > div > div"):
        a.append(x.text)
    print(a)
```



* 10페이지 뽑아오기

```python
url="https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Ahttps://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Aall%2Cp%3Aall&query=%ED%8C%8C%EC%9D%B4%EC%8D%AC&c_id=&c_name=&sm=tab_pge&kin_start="
url_l=[]
for i in range(1,101,10):
    url_l.append(url+str(i))

# 1~10 페이지에 있는 질문 모두 추출
for url in url_l:
    response=requests.get(url)
    soup=BeautifulSoup(response.text, 'html.parser')
    a=soup.select("#elThumbnailResultArea > li > dl > dt.question >a")
    for i in a:
        print(i.text)
```



* 10 페이지 뽑아오고 '초보' 단어수 세기

```python
import re

# 10페이지 뽑아오기
url="https://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Ahttps://search.naver.com/search.naver?where=kin&kin_display=10&qt=&title=0&&answer=0&grade=0&choice=0&sec=0&nso=so%3Ar%2Ca%3Aall%2Cp%3Aall&query=%ED%8C%8C%EC%9D%B4%EC%8D%AC&c_id=&c_name=&sm=tab_pge&kin_start="
url_l=[]
for i in range(1,21,10):
    url_l.append(url+str(i))
cnt=0
# 1~10 페이지에 있는 질문/답변 모두 추출
for url in url_l:
    response=requests.get(url)
    soup=BeautifulSoup(response.text, 'html.parser')
    a=soup.select("#elThumbnailResultArea > li > dl > dt.question >a")

    for link in a:
        links.append(link.attrs['href'])

    for link in links:
        url=link
        response=requests.get(url)
        text=response.text
        soup=BeautifulSoup(text,'html.parser')
        print("\nTitle")
        for x in soup.select("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__title > div > div.title"):
            t = x.text.strip('\n\t')
            print(t)
            if '초보' in t:
                a = t.count('초보')
                cnt+=a
        print("\nQuestion")
        for x in soup.select("#content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__content"):
            q = re.sub("\u200b",' ', x.text).strip('\n\t')
            print(q)
            if '초보' in x.text:
                a = q.count('초보')
                cnt+=a
        print("\nAnswer")
        for x in soup.select("#answer_1 > div._endContents.c-heading-answer__content > div._endContentsText.c-heading-answer__content-user > div > div"):
            an = re.sub("\u200b",' ', x.text).strip('\n\t')
            print(an)
            if '초보' in x.text:
                a = an.count('초보')
                cnt+=a
        print("="*100,"\n")
print(cnt)
```

