### day28_데이터분석16(200130)



##### 일정

* 문법은 거의 끝남, 앞으로 머신러닝/딥러닝 배우며 활용할 것

~ 2/1: pandas, numpy

> 이론은 2/1에 끝나지만 끝나는 건 아냐, 분야 넓기 때문에 계속 공부하고 활용해야 함

2/3~2/14: r 프로그래밍 & 머신러닝 & 자연어 처리(논문 내용까지 깊이 들어감)

> 스팸메일 분류기, 클러스터링, 결측값 대체
>
> 자연어처리(형태소 분석), 토픽모델링

2/17 ~ 2/21: 1차 프로젝트 -발표일 

> 개인적 주제 or 강사님이 주시는 주제로 진행
>
> 일주일동안 프로젝트만 하는 것, 마지막 날 오후에 발표로 마무리

2/24 ~ 2/28: Azrure 머신러닝

3/2 ~ 3/13: 파이썬 머신러닝

> 분류기(암환자), 타이타닉/자전거, 집값 예측, 전기 요금 예측

3/16 ~ 3/27: 텐서플로 & 딥러닝

> 주식예측, 숫자인식, 날씨 예측,...

3/30 ~ 4/3: CNN

> 이미지 분류: 이미지넷 대회 우승 CNN 알고리즘, 텐서플로, Keras

4/6 ~ 4/17: RNN & LSTM & 자연어처리

> RNN 구조적 문제 해결해 놓은 것이 LSTM으로 유사한 알고리즘
>
> LSTM 기반의 시계열 데이터 분석 학습 진행
>
> 자연어처리 베이즈 기반 자연어처리기법, 자연어 이해-생성, 품사 태깅 => 영화 줄거리 => 영화 추천

4/20 ~ 4/24: 챗봇 만들기

> 프로젝트 X, 같이 만들어볼 것

4/27 ~ 5/8: GAN 및 강화학습

> 유사이미지 생성, 자율주행차 시뮬레이터

5/11 ~ 5/21: 2차 프로젝트 - 발표일

> 취업 생각하면 CNN, RNN, 데이터분석부분에서 프로젝트 하는 것 추천
>
> > 타겟 회사에서 담당하는 업무와 관련된 프로젝트 진행해야
>
> GAN, 강화학습은 아직 취업시장 작아 대신 IT업계에서 갖춰야 할 소양

5/21 ~ 6/30: 프로젝트 관리 - 새로운 강사님과 수업





#### 자료형 처리

```python
mport pandas as pd
import seaborn as sns
```



##### 자료형 변환

```python
tips = sns.load_dataset('tips')

tips.info()
```

* category형으로 되어있는 sex 문자형으로 변환

```python
tips['sex_str'] = tips['sex'].astype(str)
tips.dtypes
# > sex_str         object
```

* head 부분만 추출, 저장해서 변환

```python
tsm = tips.head(10)
tsm.loc[[1,3,5,7],'total_bill']='missing'
tsm
```

![tsm](0130_images/tsm.png)

```python
tsm.dtypes
# > total_bill float64 > object로, 문자가 들어가있기 때문에 모두 문자로 처리됨
```

* object인 total_bill 숫자형으로 바꾸기

> errors 옵션
>
> 'ignore'옵션지정: 문자(에러) 무시하고 실행, 아무 작업 x
>
> > total_bill     object
>
> 'coerce'옵션지정: 문자(에러) 무시, 숫자해당하는 것만 변환 > 수치 타입으로
>
> coerce: 숫자로 변환할 수 없는 값은 NaN으로 지정
>
> > total_bill     float64
>
> 'raise': 디폴트로 숫자로 변환할 수 없는 값 있으면 에러 발생



##### 카테코리 자료형

> 카테고리 자료형이 소요되는 비트수 적어 용량 적어 유용, 속도 빠름

* category, str 자료형 용량 비교

```python
# 카테고리 자료형 str로 변환

tips['sex'] = tips['sex'].astype('str')
tips.info()

tips['sex'] = tips['sex'].astype('category')
tips.info()
```

![용량비교](0130_images/용량비교.png)



##### 문자열

```python
word = 'hello'
sent = 'world'

# 문자형 추출
sent[-1] # 음수범위로 인덱싱 가능 > 맨 마지막 문장
sent[0:3]
sent[2:-1] # > 'rl': 맨 마지막 전까치 추출되는 것 유의해야
sent[::2] # > 'wrd'
```

###### 문자열 메서드

> - capitalize(): 첫 글자를 대문자로 만들어주는 함수
> - lower(): 모두 소문자로 바꿔주는 함수
> - upper(): 모두 대문자로 바꿔주는 함수
> - count('특정글자'): 특정 글자 몇 번 나오는지 카운트
> - startswith('특정글자,단어'): 해당 글자, 단어로 시작합니까 묻는 함수로 맞으면 True
> - endswith(''): 해당 글자, 단어로 끝나는지 묻는 함수로 맞으면 True
> - find('특정 글자'): 해당 글자를 처음 만나는 자리의 인덱스값 반환
> - index('특정 글자'): 해당 글자를 처음 만나는 자리의 인덱스값 반환
>     - fidn에서는 검색 안되면 -1반환, index는 해당글자 없으면 에러
> - isalpha(): 알파벳으로만 구성되어있는지 묻는 함수로 공백이라도 있으면 False 반환
> - isdecimal(): 숫자로 구성되어있는지 확인하는 함수
> - replace('기존 문자','바꿀 문자'): 기존 문자를 바꿀 문자 지정해서 바꿔줌
> - strip(): 양쪽 공백 없애주는 함수
> - split(sep=' '): sep으로 지정해준 특징을 기준으로 문자열 분리해서 리스트로 반환

```python
'Hello World'.capitalize()
'Hello World'.count('o')
'Hello World'.startswith('H')
'Hello World'.endswith('World')
'Hello World'.find('l')
'Hello World'.find('z') # -1
'Hello World'.index('l')
# 'Hello World'.index('z') # error
'Hello World'.isalpha() # 공백있으므로 False
'HelloWorld'.isalpha() # 공백없애면 True

'130'.isdecimal() # True
"I'm 25".isdecimal() # False

'Hello World'.lower()
'Hello World'.upper()
'Hello World'.replace('Hello','Hi')
'   Hello World   '.strip()
'Hi Hi Hi'.split(sep=' ')
```

###### 문자열 연결

> join: 문자열 연결 함수

```python
d1 = '40'
d2 = '50'
d3 = '60'
d4 = 'Number'
" ".join([d1,d2,d3,d4]) # 앞에 지정해준 특징으로 리스트 안에 문자열들 연결해줌

#> '40 50 60 Number'
```

> splitlines: 여러 줄로 구성된 문자열을 분리시켜주는 함수로 줄마다의 내용을 list 요소로 하나하나 넣어줌

```python
mystr = """
A: Hello
B: Hi
A: How are you?
B: Fine
"""
msr = mystr.splitlines() 
msr
# > ['', 'A: Hello', 'B: Hi', 'A: How are you?', 'B: Fine']
```

###### 포매팅

```python
a = 'hi'
s = 'hello {}'
s.format(a)
# > 'hello hi'


myhome='location: {lat},{lon}'
myhome.format(lat='36',lon='125')
# > 'location: 36,125'


s = '%d digits of pi' %3
s
# > '3 digits of pi'


print('some digits of %(co)s: %(val).2f' %{'co':'e','val':2.718})
# > some digits of e: 2.72
```



###### apply 함수

> apply메서드에 함수 적용하는 법, 잘 알아두기!

```python
df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})
df

def my_sq(x):
    return x**2
def my_exp(x,n): # 인수 2개로 구성되어 있어야
    return x**n
```

* 시리즈에 적용

```python
df['a'].apply(my_sq)
df['a']**2 # 동일한 결과 출력
# 0    1
# 1    4
# 2    9
# Name: a, dtype: int64


# df['a'] 데이터는 시리즈이기 때문에 인수 2개인 함수에선 지정해줘야
df['a'].apply(my_exp,n=3) # 3승
# 0     1
# 1     8
# 2    27
# Name: a, dtype: int64
```

* 데이터 프레임에 적용

```python
def print_me(x):
    print(x)
```

```python
print(df.apply(print_me)) # axis=0이 default

# 0    1
# 1    2
# 2    3
# Name: a, dtype: int64
# 0    2
# 1    3
# 2    4
# Name: b, dtype: int64
# a    None
# b    None
# dtype: object
```

```python
print(df.apply(print_me,axis=1))

# a    1
# b    2
# Name: 0, dtype: int64
# a    2
# b    3
# Name: 1, dtype: int64
# a    3
# b    4
# Name: 2, dtype: int64
# 0    None
# 1    None
# 2    None
# dtype: object
```

* 인수개수 많을 때 적용

```python
def myavg(x,y,z):
    return (x+y+z)/3


# 행의 개수(=> 인수개수) 안다는 가정하에
def myavg_apply(data):
    x=data[0]
    y=data[1]
    z=data[2]
    print(x)
    return (x+y+z)/3


# 행의 개수 모를 때
def myavg_apply2(data):
    sum = 0
    for item in data:
        sum += item
    return sum/df.shape[1]
```

```python
df.apply(myavg_apply2)
# a    2.0
# b    3.0
# dtype: float64


# myavg_apply2의 df.shape[0] > df.shape[1]로 바꿔줘야
df.apply(myavg_apply2, axis=1) 
# 0    1.5
# 1    2.5
# 2    3.5
# dtype: float64
```



* titanic data로 apply 활용

> 누락값의 개수 리턴해주는 함수, 누락값 비율 구하는 함수 만들기
>     - 각 컬럼별로 누락값, 누락값 비율 구해서 리턴해줘야 함
>         - 판다스함수, 넘파이 함수 둘 다 잘 응용, 적용할 수 있어야

```python
titanic = sns.load_dataset('titanic')

# 누락값 개수구하는 함수
def count_missing(vec):
    nv = pd.isnull(vec)
#     print(nv) # > 각 열별로 True,False 리턴됨
    nc = np.sum(nv) # sum()에서 True:1, False:0으로 계산
    return nc

# 누락값 비율 구하는 함수
def prop_missing(vec):
    num = vec.size
#     print(vec.size)
    miss = count_missiing(vec) # 위에서 만든 count_misiing함수 호출해서 결측값 개수 가져옴
    return miss/num


def prop_complete(vec):
    return 1-prop_missing(vec)
```

* 열방향 누락값 개수 조사

```python
cm = titanic.apply(count_missing)
print(cm) # > 각 컬럼별 missing data 개수가 모두 출력

# survived         0
# pclass           0
# sex              0
# age            177
# sibsp            0
# parch            0
# fare             0
# embarked         2
# class            0
# who              0
# adult_male       0
# deck           688
# embark_town      2
# alive            0
# alone            0
# num_missing      0
# dtype: int64
```

```python
pm = titanic.apply(prop_missing)
print(pm)
# survived       0.000000
# pclass         0.000000
# sex            0.000000
# age            0.198653
# sibsp          0.000000
# parch          0.000000
# fare           0.000000
# embarked       0.002245
# class          0.000000
# who            0.000000
# adult_male     0.000000
# deck           0.772166
# embark_town    0.002245
# alive          0.000000
# alone          0.000000
# num_missing    0.000000
# dtype: float64
```

```python
pcm = titanic.apply(prop_complete)
print(pcm)
# survived       1.000000
# pclass         1.000000
# sex            1.000000
# age            0.801347
# sibsp          1.000000
# parch          1.000000
# fare           1.000000
# embarked       0.997755
# class          1.000000
# who            1.000000
# adult_male     1.000000
# deck           0.227834
# embark_town    0.997755
# alive          1.000000
# alone          1.000000
# num_missing    1.000000
# dtype: float64
```

* 행방향으로 누락값 개수 조사 

```python
cmr = titanic.apply(count_missing,axis=1) # 각 행별 누락값 조사
pmr = titanic.apply(prop_missing,axis=1)
pcmr = titanic.apply(prop_complete,axis=1)



cmr.head()
# 0    1
# 1    0
# 2    1
# 3    0
# 4    1
# dtype: int64

pmr.head()
# 0    0.0625
# 1    0.0000
# 2    0.0625
# 3    0.0000
# 4    0.0625
# dtype: float64

pcmr.head()
# 0    0.9375
# 1    1.0000
# 2    0.9375
# 3    1.0000
# 4    0.9375
# dtype: float64
```

* 누락값 개수 행 추가

```python
titanic['num_missing'] = titanic.apply(count_missing,axis=1)
titanic
```

![누락값추가](0130_images/누락값추가.png)

* num missing 값이 1보다 큰 행 추출

```python
# titanic[titanic['num_missing'] > 1]
titanic.loc[titanic.num_missing > 1]
```

* sample(): 임의로 표본추출하는 함수

```python
titanic.loc[titanic.num_missing > 1].sample(10)
```





#### 데이터 분석

* 미국 통계청 이름 자료로 어떤 이름이 유행이었는지 분석

> names data: 미국 통계청에서 130년동안 조사한 자료
> 가장 인기 많은 이름, 이름 트렌드, 패턴변화, 규칙 패턴 찾아낼 수 있음

```python
names2010 = pd.read_csv('names/names/yob2010.txt', names=['name','sex','births'])
names2010
```

![names_df](0130_images/names_df.png)

```python
# data 건수 확인

names2010.info()
names2010.shape[0] # 33838
```

* 성별 기준으로 births의 합계 출력

```python
names2010.groupby('sex').sum()
```

![성별기준births합계](0130_images/성별기준births합계.png)

```python
names2010.groupby('sex').births.sum()

# sex
# F    1759010
# M    1898382
# Name: births, dtype: int64
```



##### 파일들 다 합치기

* names에 있는 파일 다 불러와서 데이터프레임을 리스트에 요소로 저장하기

```python
"""
- names에 있는 파일 다 불러와서 130개의 데이터프레임을 리스트에 요소로 저장 
[1880df, 1881df, ..., 2010df]
"""

pieces = []
for year in range(1880, 2011):
    path = 'names/names/yob%d.txt' % year
    #rint(path)
    df = pd.read_csv(path,names=['name','sex','births'])
    df['year']=year # 구분 이하게 열 추가해주는 것
    pieces.append(df)
    
"""
list에 저장되어있는 130개의 데이터프레임을 하나의 데이터프레임으로 합치고 싶을 때: concat 사용
"""    
# pd.concat(pieces)
# pd.concat(pieces).shape # (1690784, 4)

# 인덱스에 일련번호 부여
names = pd.concat(pieces, ignore_index=True)


names.head(20)
# names[:20]
```

![names파일들합친df](0130_images/names파일들합친df.png)

##### pivot

> births를 데이터, 행인덱스를 year, 열을 sex로 pivot table
> pivot table > 평균값 나타내줌: aggfunc=mean이 default

```python
names.pivot_table(values='births', index='year', columns='sex')

# 합계 구하는 함수 적용 
total_births = names.pivot_table(values='births', index='year', columns='sex',aggfunc=sum)
# pd.pivot_table(data=names,values='births', index='year', columns='sex',aggfunc=sum)

total_births.tail()
```

![pivot1](0130_images/pivot1.png)

* 출생자수 변화 시각화

```python
total_births.plot(title='Total births')
```

![출생지수변화시각화](0130_images/출생지수변화시각화.png)



##### 이름이 차지하는 비중 컬럼 추가

> names에서 각각의 이름이 차지하는 비중 컬럼으로 추가하기
> 분모: 태어난 아이들 수/ 분자: 이름 개수
>
> 1. 그룹화: year,sex를 기준으로 그룹화
> 2. 비중 출력: 각 해마다 그 해 연도 남/녀 아이들 중 해당 이름이 차지하는 비중을 구하는 것
>              prop 합계는 1이 된다.
>
> ex) 1880년 Mary라는 이름은 1880년에 태어난 여자아이들 중 전체에서 차지하는 비중이 얼마냐?
>     위와 같은 작업을 모든 이름에서 각 연도마다 조사하는 것

```python
def add_prop(group):
    births = group.births.sum() # 연도별 태어난 남/녀 아이들 수 합계 > 분모로
    group['prop'] = group.births/births
    return group
    
names = names.groupby(['year','sex']).apply(add_prop)
names.head()
```

![prop추가df](0130_images/prop추가df.png)

* np.allclose()

```python
np.allclose(names.groupby(['year','sex']).prop.sum(),1)
# > True
```



##### 이름 빈도수 조사

```python
# 각 연도별, 성별에 따른 이름 빈도수가 가장 높은 이름을 1000개씩 추출

def get_top1000(group):
    return group.sort_index(by='births', ascending=False)[:1000]

grouped = names.groupby(['year','sex']) # 260개 그룹 객체
top1000 = grouped.apply(get_top1000) # 260개 그룹 객체에 함수 적용

# 그룹인덱스 제거
top1000.reset_index(inplace=True, drop=True)
top1000[:20]
```

![빈도수조사df](0130_images/빈도수조사df.png)

* top1000 성별나누어 저장

```python
girls = top1000[top1000.sex=='F']
boys = top1000[top1000.sex=='M']
```

![top1000_girls](0130_images/top1000_girls.png)

* pivot

```python
total_births = top1000.pivot_table(values='births',index='year',columns='name',aggfunc=sum)
total_births
```

![top11000_girls_pivot](0130_images/top11000_girls_pivot.png)



##### 상위 50%에 해당하는 이름 개수 조사

> 해당 데이터는 같은 이름 5명까지 사용하는 경우를 담게 된다.
>
> - cumsum 사용해서 상위 50%에 해당하는 이름 개수 알아보기
> ex) 2010년 상위 50% 이름 개수 100여개
>     1880년 사우이 50% 이름 개수 20여개
>     ==> 연도가 거듭할수록 이름의 종류가 다양해졌다는 패턴 확인 가능

```python
df = boys[boys.year==2010]
df
```

![top1000_boy_2010](0130_images/top1000_boy_2010.png)

* cumsum()

```python
df.sort_index(by='prop',ascending=False).prop.cumsum()
# 260877    0.011523
# 260878    0.020934
# 260879    0.029959
# 260880    0.038930
# 260881    0.047817
#             ...   
# 261872    0.842748
# 261873    0.842850
# 261874    0.842953
# 261875    0.843055
# 261876    0.843156
# Name: prop, Length: 1000, dtype: float64

prop_cumsum = df.sort_index(by='prop',ascending=False).prop.cumsum()
```

> 5개 이상 이름 누적합: 약 84% => 16%는 동명이인 5명이 안 되는 것 >> 이름 다양해진다는 것 유추 가능

* prop_cumsum.values.searchsorted

```python
prop_cumsum.values

# 누적합이 0.5인 지점까지 이름 종류의 개수
prop_cumsum.values.searchsorted(0.5) # 116
```

***

위의 과정 for문 돌려서 해마다 비교하면서 이름 종류 다양해졌는지 확인 가능

***

```python
def get_qc(group,q=0.5):
    group = group.sort_index(by='prop', ascending=False)
    return group.prop.cumsum().values.searchsorted(q)

res = top1000.groupby(['year','sex']).apply(get_qc)
print(res)
res.plot()
```

![res1](0130_images/res1.png)

* 결과 stack해주기

```python
res = res.unstack('sex')

res.plot()
res
```

![res2](0130_images/res2.png)



#### 연습문제

> 연습문제1. 
>
> 1900, 1950, 2010년 성별로 구분, 전체 이름을 구성하는 알파벳 문자 비율 조사
> ex) 
> 1900년 남자아이들 이름 > 알파벳 카운트 > 각 알파벳별 몇번 들어갔는지 출력
> john
> micheal
> ...
> a: 10만건, b:9만건, ..., z:3천건
>
> 1900년 남자아이들 이름에 가장 많이 사용된 알파벳 문자?
> 1950년 남자아이들 이름에 가장 많이 사용된 알파벳 문자?
> 2010년 남자아이들 이름에 가장 많이 사용된 알파벳 문자?
>
> 1990년 여자아이들 이름에 가장 많이 사용된 알파벳 문자?
> 1950년 여자아이들 이름에 가장 많이 사용된 알파벳 문자?
> 2010년 남자아이들 이름에 가장 많이 사용된 알파벳 문자?



