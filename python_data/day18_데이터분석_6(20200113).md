### day18_데이터분석6(20200113)



* 이번주 금요일 데이터 분석 프로젝트
* 다음주는 23일에 프로젝트
* 이번주 판다스, 넘파이 문법 다루고 다음주까지 데이터분석 진도나감




```python
import pandas as pd
import numpy as np
```

* 복습

```python
df=pd.DataFrame(np.random.randn(5,3),columns=['c1','c2','c3'])

# 0,0위치에 None 집어넣기
# df.ix, df.loc, df.iloc로 표현 가능
df.ix[0,0]=np.nan

# 특정 데이터 none값으로 만들기
df.ix[1,['c2']]=None
df.ix[2,['c2']]=None
df.ix[3,['c2']]=None
df.ix[4,['c3']]=None


# NaN값 채우기
# NaN이 들어가있는 자리를 모두 0으로 대체
df.fillna(0)
# ffill 사용
df.fillna(method='ffill')
```

```python
# NaN 평균값으로 채우기
df.mean
df.fillna(df.mean())

# c1값의 평균값으로 데이터프레임 전체 NaN을 대체해라
df.fillna(df.mean()['c1'])

# 평균값 구하기
df.mean()['c1']
df.mean()['c1':'c2']
df.fillna(df.mean()['c1':'c2']) 
# 지정한 특정 열만 평균값으로 NaN값 대체, C3은 대체 x > NaN값으로 출력
```



#### where 문법

* where사용해서 특정값 대체 가능

* where사용해서 NaN 평균값으로 채우기

```python
df.where(pd.notnull(df), df.mean(), axis='columns') 
# axis='columns': notnull이 아닌 것은 평균값으로 대체하라는 의미로 결측값 대체할 떈 이런식으로 작성 가능
```



* numpy에서도 where문법 사용 가능
  * where문 사용: where함수는 조건문 사용 -> 출력값: 인덱스
  * 형식: np.where(조건, 참, 거짓)

```python
arr=np.array([1,2,3,10,20,30,0,1,0,2])

# arr에서 최소값 출력하기
np.min(arr)
# arr에서 최대값 출력하기
np.max(arr)

# argmin(): 최소값이 저장되어 있는 인덱스(위치) 구할 때 사용하는 함수
np.argmin(arr) # 최소값의 index
# argmax(): 최대값이 저장되어 있는 인덱스(위치) 구할 때 사용하는 함수
np.argmax(arr) # 최대값의 index
```

```python
# where문 사용: where함수는 조건문 사용 -> 출력값: 인덱스
np.where(arr<1)
# whrer문 슬라이싱처럼 사용 가능
# 형식: np.where(조건, 참, 거짓)
np.where(arr>=10,0,arr) # arr에서 10이 넘는 참인 조건은 0으로 바꾸고 아닌 것(거짓)은 arr로 출력
np.where(arr>=10,0,1)
np.where(arr>=10, arr*0.1, arr)
```



* where문 사용해서 새로운 열 추가

```python
df=pd.DataFrame({'c1':[1,2,3,4,5],
                'c2':[6,7,8,9,10]})

df.ix[[1,3],['c2']]=np.nan
df

"""
c2p라는 새로운 열 추가하기: c2열 값이 notnull이면 c2열 값을, c2열 값이 null이면 c1열 값으로 채우고자 함
"""

# df['c2p']=np.where(조건,참,거짓)
df['c2p']=np.where(pd.notnull(df['c2'])==True,df['c2'],df['c1'])
# pd.notnull(df['c2']==True: c2열 값이 null이 아닌 것이 참인 값 == null이 아닌 값 있는 c2열
df
```



* 위의 구문을 반복문, for문으로 변경하기
  * where문 사용하는 것보다 훨씬 복잡함

```python
for i in df.index:
#     print(i)
#     print(df.ix[i,'c2']) ==> 각 행에 대한 c2열 확인 가능
    if pd.notnull(df.ix[i,'c2'])==True:
        df.ix[i,'c2p']=df.ix[i,'c2']
    else:
        df.ix[i,'c2p']=df.ix[i,'c1']
df
```



#### dronpna 메서드

* 결측값이 있는 열(axis=1)/행(axis=0) 제거

```python
df=pd.DataFrame({'c1':[1,2,3,4,5],
                'c2':[6,7,8,9,10]})
df.ix[[1,3],['c2']]=np.nan
df

# 결측값 있는 행,열 제거하기
df.dropna(axis=0) # 결측값이 있는 행 전체 제거
df.dropna(axis=1) # 결측값이 있는 열 전체 제거

# 원하는 특정열에서 결측값있는 행 전체 제거
df[['c1','c2']].dropna() # df.dropna(axis=0) > axis=0: default
```




#### 결측값 보간(interpolate) 

: 선형적으로 비례하게 결측값을 대체함

 1) 시계열데이터에 대해 선형적으로 비례하는 값
    	ex. 0 2 4 Nan => Nan을 6으로    

 2) 이미지보간 : 그라데이션과 비슷 

* 이미지데이터도 현재 픽셀과 다음픽셀 사이에 관계가 있기 때문에 결측값 보간방법 잘 쓰임



* 시계열 데이터

```python
from datetime import datetime
# datetime 모듈에 있는 datetime 함수 가져오기
dateStr=['1/13/2020','1/16/2020','1/17/2020','1/20/2020']
# 데이터가 문자열로 저장되어 있음

# 문자열 데이터를 날짜형데이터로 바꿔줌
dates=pd.to_datetime(dateStr)
dates
```

* 결측값 보간

```python
ts=pd.Series([1,np.nan, np.nan, 10],index=dates)

# 결측값 보간
tslr=ts.interpolate()
tslr
```





#### replace

- 결측값/실측값 대상 데이터 교체할 때 사용하는 함수

* na 대체: fillna, replace
  * fillna: na전용으로 na를 다른 값으로 대체
  * replae: fillna보다 사용 범위가 넓다. na 뿐만 아니라 다른 모든 값에 대해서도 대체 가능
    
     * fillna보다 광범위하게 사용
     
     


* series에서 replace 적용

```python
s=pd.Series([1,2,3,4,np.nan])
# replace: 결측값/실측값 변경
# 실측값 변경: 3 => 9
s.replace(3,9) 
# 결측값 변경: NaN => 5
s.replace(np.NaN,5)


# replace로 여러개 변경할 때 list형태([])로 사용
s=pd.Series([1,2,3,1,np.nan])
s.replace([1,2,3],[6,7,8]) 
# 1:1로 생각, 1은 모두 6으로 => 한번에 값 다른 값으로 변경할 때 용이
s.replace([1,2,3,np.nan],[10,20,30,99])


# replace: {} 딕셔너리 형태로 사용
s.replace({1:5}) # {원래값:변경값}
# 여러개 값 변경할 때
s.replace({1:5,3:30,np.nan:99}) 
```



* 데이터프레임에서 replace 적용

```python
df=pd.DataFrame({'c1':['aaa','b','c','d'],
                 'c2':[1,2,3,4],
                 'c3':[5,6,7,np.nan],})

df.replace({'c1':'aaa'} ,{'c1':'bbb'})
df.replace({'c3':np.nan} ,{'c3':99})
```





#### 중복데이터 처리

* 병합: 중복 데이터 발생 => 중복데이터 처리해줘야 함!

- duplicated()

  : 중복데이터 여부 확인하는 함수

- drop_duplicates()

  : 중복데이터 처리하는 함수로 유니크한 한개만 남겨놓고 나머지는 제거

  

  ex)    id               id
          a01           a01
                           a01    

  ​                         a02
  ​                         a01            

  ==> 중복데이터 많이 발생 : 데이터 분석에서 큰 문제점으로 발생할 수 있음

  ex. a01 홍길동 -> 임꺽정으로 바꿀 때 중복데이터 중 일부만 바뀌는 경우 생겨

* duplicated()

```python
df=pd.DataFrame({
    'k1':['a','b','b','c','c'],
    'k2':['x','y','y','x','z'],
    'col':[10,20,30,40,50]})

# 중복 데이터 확인
df.duplicated(['k1'])
df.duplicated(['k2'])
df.duplicated(['k1','k2'])

# keep 옵션 사용
"""
True: 중복이 되었다
False: 중복이 안 되었다
"""
df.duplicated(['k1'],keep='first') # keep='first': defalut로 설정, 맨 처음에 나오는 것만 중복 아닌걸로 보고 나머지는 True로
df.duplicated(['k1'],keep='last') # 마지막 중복을 False로 보고 나머지는 True(중복)로 보겠다
df.duplicated(['k1'],keep=False)  # 중복은 모두 중복(True)으로
```

* drop_duplicates()

```python
# 중복값 제거: 1개만 살리고 나머지 중복은 제거

df.drop_duplicates(['k1'],keep='first') # default
df.drop_duplicates(['k1'],keep='last')
df.drop_duplicates(['k1'],keep=False) # 다 제거
```



#### unique()와 value_counts()

* unique()
  * 유일한 값을 찾을 때 사용하는 함수
* value_counts()
  * 유일한 값 개수를 셀 때 사용하는 함수

```python
df=pd.DataFrame({
    'a':['a1','a1','a2','a2','a3'],
    'b':['b1','b1','b2','b2',np.nan],
    'c':[1,1,3,4,4]})
```

* unique()

```python
df['a'].unique()
df['b'].unique()
df['c'].unique()
# help(pd.Series.unique)
```

* value_counts(): 유일값 개수 세기
  * normalize=디폴트(False): 개수, True: 상대비율로 출력
  * sort 옵션
    * sort=True: 개수 기준 정렬로 default값
    - sort=False: 정렬기준이 없음
  * ascending 옵션
    * sort=True > ascending=True: 개수 기준으로 오름차순 정렬
    - sort=True > ascending=False: 개수 기준으로 내림차순 정렬 => default
  * default: sort=True, ascending=False
    
    - 유일한 값의 개수를 기준으로 내림차순

```python
help(pd.Series.value_counts)
# normalize=디폴트(False): 개수, True: 상대비율로 출력
df['a'].value_counts()

df['a'].value_counts(normalize=False,sort=True, ascending=True)
df['a'].value_counts(normalize=False,sort=True, ascending=False)

df['c'].value_counts(sort=True, ascending=False)
# default: sort=True, ascending=False
# 유일한 값의 개수를 기준으로 내림차순

df['c'].value_counts(sort=True, ascending=True)
df['c'].value_counts(sort=False) # 정렬기준이 없음
```



* value_counts() 옵션
  * dropna
  * bins

```python
df['b'].value_counts()
# default: dropna=True: NA 제외시키고 나머지 데이터들로 value_counts 실행
df['b'].value_counts(dropna=False) # dropna=False: na값도 세게 하는 옵션



df['c'] # data: 1 1 3 4 4
# group별 데이터 개수 세기
df['c'].value_counts(sort=False)
"""
bins 옵션: bins 구간 나타내며 구간별로 나누어 구간그룹별 데이터를 셀 수 있다. 
(): 개구간, []:폐구간 (-0.001, 1.0] 2 
=> 개구간:(으로 시작하는 -0.001은 포함 x, 폐구간:]로 끝나는 1.0은 포함
"""
df['c'].value_counts(bins=[0,1,2,3,4,5],sort=False)

# pandas cut 함수로 동일한 결과 도출 가능
# 구간을 나타낼 때 사용하는 cut 함수
res=pd.cut(df['c'],bins=[0,1,2,3,4,5])
pd.value_counts(res) # df['c'].value_counts(bins=[0,1,2,3,4,5],sort=False) 코드와 동일한 결과
```





#### 표준화

- 변수들 간 척도가 다른 경우 데이터 표준화 작업을 해줘야 함, 모수의 왜곡이 생기지 않도록
    
- ex. 만점 척도 다른 토플과 토익
    
- 표준화: (각 데이터-평균)/표준편차
    
    - 표준화=(각 데이터 - 평균(각 열))/표준편차(=std(각 열))
        - 표준편차: np.std()
    - 모집단이 정규분포를 따르는 경우에, 평균:0, 표준편차:1인 표준정규분포로 표준화하는 작업
    - 표준화하는 방법
    1) numpy
        2) scipy.stats:zcore
        3) sklearn.preprocessing 함수 사용
    
    

##### 1. numpy 사용

```python
from numpy import *

data=np.random.randint(30,size=(6,5))
data
"""
data0과 거리가 가장 작은 데이터는? 
피타고라스정리(2차원)->유클리디안 거리(다차원)
 ==> 특정축에 영향을 크게 받는 문제가 발생해 표준화를 하게 되는 것
표준화: 각 열에 대해서 평균과 표준편차를 구하여 표준화를 하여 평균으로부터 얼마나 떨어져있는지 비교 가능하게 만든다.
"""
```

```python
"""
data 표준화하기
표준화=(각 데이터 - 평균(각 열))/표준편차(=std(각 열))
표준편차: np.std()
"""
np.mean(data) # 전체 평균
np.mean(data, axis=0) # 각 열 평균
np.mean(data, axis=1) # 각 행 평균

# 각 데이터 - 평균(각 열)
"""
broadcasting: 내부적으로 이루어지는 두 대상의 shape을 맞춰주는 작업
=> 3행 2열에서 2열 값을 빼줘야하는 경우 2열이 3행 2열로 복사가 되어서 2차원으로 이어져 빼기 수행됨.
=> 각 열 평균 값(np.mean(data, axis=0))이 브로드캐스팅작업으로 데이터 shape 맞춰지는 작업이 먼저 행해짐.
=> 해당 데이터에 있는 값(=요소) 간 빼기
"""

# 각 열 평균 값(np.mean(data, axis=0)) 구하기
print(data)
print("="*60)
print(np.mean(data, axis=0))
print("="*60)
print(data-np.mean(data, axis=0))

# 표준편차(=std(각 열)) 구하기
print("="*60)
np.std(data) # 전체 데이터에 대한 표준편차
np.std(data, axis=0) # 각 열에 대한 표준편차
print(np.std(data, axis=0))

# 표준화: (각 데이터 - 평균(각 열))/표준편차(=std(각 열))
print("="*60)
std_data=(data-np.mean(data, axis=0)) / np.std(data, axis=0)
print(std_data)
```



##### 2. scipy 사용

```python
import scipy.stats as ss

# ss.zscore()로 한번에 데이터 표준화할 수 있음
data_ss=ss.zscore(data)
data_ss
```



##### 3. sklearn 사용

```python
from sklearn.preprocessing import *

ss_data=StandardScaler().fit_transform(data)
ss_data
```







#####  예외적인 값 포함된 경우 표준화 작업

* 예외적인값: 이상치, 특이값, outlier

* 표준정규분포로의 표준화: 이상치, 특이값이 없어야한다는 가정 필요

  - 표준편차 공식=(x-mean)/std  

    - 이상치, 특이값 없다는 전제 깔려있음
    
    - 특이값 있다면 데이터 정규화로 변환하는 과정에서 특이값이 평균, 표준편차에 큰 영향을 미친다.

    - mean과 std가 특이값, 이상치에 민감하다

          - 특이값, 이상치 있으면 제대로 된 표준화 만들지 못한다.

        

- 이상치가 데이터에 포함되어 있는 경우에는 표준화를 어떻게?
    
    1) 이상치, 특이값을 찾아서 제거 : 모수적 방법에서 주로 사용
    
       - IQR 사용
    
            :3-4분위수에서 1-2분위수를 빼주고 1.5배를 한다음 그 범위 벗어나는 데이터 빼줌
    
    - IQR 사용하면 크게 벗어나는 범위(특이값) 찾을 수 있음(:조건문으로 하나하나 찾아내야) > 특이값 제거 > 나머지로 표준화
    
    2)  중앙값(median), IQR을 이용하여 스케일링 : 비모수적 방법에서 주로 사용
    
    * 특이값, 이상치에 민감한 mean과 std 대신 중앙값(median), IQR을 이용
        * mean 대신 median 사용
        * std 대신 IQR 사용



* RobustScaler() 사용
  * 이상치가 있는 데이터 표준화해주는 클래스로, sklearn에 포함

```python
# RobustScaler(): 이상치가 있는 데이터 표준화해줌

from sklearn.preprocessing import StandardScaler, RobustScaler 
# RobustScaler import
import matplotlib.pyplot as plt
```

* 정규분포 따르는 난수 데이터 생성

```python
mu, sigma = 5, 2

np.mean(np.random.randn(100)) 
# randn: n=normal distribution으로 정규분포 따르는 난수 나온다
x = mu + sigma*np.random.randn(100)


# 정규분포 따르는 난수 데이터 확인해주는 작업
plt.hist(x)
np.mean(x)
np.std(x)
```

* 정규분포 따르는 난수 데이터에 인위적으로 이상치 넣어주기

```python
# RobustScaler() 사용하기 위해 위의 데이터에 인위적으로 이상치 넣어주기
x[98:100]=100
np.mean(x)
np.std(x)

plt.hist(x)
plt.hist(x, bins=np.arange(1,102,2)) # 구간을 2씩 나눠줌
```

* 이상치 포함 데이터 StandardScaler()로 표준화하기 위한 reshape 작업

```python
x
x.shape # (100,)
# x의 자료구조: array <= numpy로 만들어줌
# StandardScaler().fit_transform(x)  ==> Error: Expected 2D array


# (100,) => (100,1) 데이터로 바꿔줘야

# reshape 사용: x가 numpy로 만들어진 array 자료이기에 가능
# 1
x=x.reshape(100,1)
x.shape # (100, 1)
# 2
x=x.reshape(-1,1) 
x.shape # (100, 1)
```

* 이상치 포함 데이터 StandardScaler로 표준화 해주기

```python
# 표준화(이상치 포함 데이터)
ss_x=StandardScaler().fit_transform(x)
np.mean(ss_x) # -1.7763568394002505e-17: 0에 가까운 값
np.std(ss_x) # 1.0
plt.hist(ss_x) 
# 그림으로 확인하면 이상치로 인해 제대로 된 정규화 아니라는 것 알 수 있음
```

![이상치포함정규분포](images/이상치포함정규분포.png)

* 이상치 제거하고 정규분포 나타내기

```python
# boolean 참조 사용
ss_x_z = ss_x[ss_x<5]

# plt.hist(ss_x_z) # 이상치 제거 후 > 종모양의 정규분포 나옴
plt.hist(ss_x_z, bins=np.arange(-1,1,0.1)) 
# 범위 지정해서 그래프 나타낼 수 있음
```

![standardscaler](images/standardscaler.png)



* RobustScaler() 사용: 이상치 포함 데이터의 중앙값, IQR 이용해서 표준화하기

```python
np.median(x)
q1 = np.percentile(x,25) # 1사분위수: 25번째 위치, 3.855
q3 = np.percentile(x,75) # 3사분위수, 6.167
iqr = q3 - q1 #  6.167 - 3.855 = 2.312


# RobustScaler() 사용
# RobustScaler(): 각 데이터에서 중위수 뺀 값을 iqr로 나눈 값 수치 나타내주는
x_rs = RobustScaler().fit_transform(x)
x_rs

np.median(x_rs) # 1.9212062496443139e-16: 0로 중위수 0
np.mean(x_rs) # 0.763
np.std(x_rs) # 5.809
plt.hist(x_rs)

x_rs_z = x_rs[x_rs < 10]
plt.hist(x_rs_z,bins=np.arange(-1,1,0.1))
```

* RobustScaler() 사용한 값에서 이상치 제거해준 후 정규분포 나타내기

```python
x_rs_z = x_rs[x_rs < 10]
plt.hist(x_rs_z,bins=np.arange(-1,1,0.1))
```

![robustscaler](images/robustscaler.png)





용어정리

- 모집단: 전체 집단
- 모집단에서 표본추출 -> 표본집단 평균, 표준편차... -> 통계량 => 모평균, 모분산(=모수, parameter) 추정
  - 모수를 루정하고자 하는 것이 궁극적 목표
 - 모수적 방법
   - 중심극한정리: 분포에 상관없이 무작위로 복원추출하면, 연속형자료의 평균에 대한 분포는 정규분포를 띈다면
   - 30개 이상의 표본의 경우에 정규분포를 따른다. 
- 비모수적 방법
  - 10개 미만 표본의 경우로 표본이 작을 때는 모수적 방법을 사용 X
  - 자료를 크기로 나열 -> 순위 매김 => 차이 비교