### day19_데이터분석7(200114)



* 파이썬 라이브러리를 활용한 데이터분석  7장,8장,10장으로 복습



#### 정규화

: **0~ 1사이의 범위로 데이터 스케일링, 표준화**

* 각 데이터 값-최소값=(분자) / 최대값 - 최소값(=분모)
  * 5 3 1 7 9 => 0~1 
    5-1 / 9-1 => 0.5
    3-1 / 9-1 => ...
    1-1 / 9-1 => 0
    ...
    9-1 / 9-1 => 1

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler # MinMaxScaler:정규화
```

* 정규화할 데이터 생성

```python
data=np.array([[10,-10,2],
              [5,0,6],
              [0,7,4]])
```

* 수식으로 정규화하기

```python
data.max() # data 배열 전체에서 최대값
data.max(axis=0) # data 각 열 단위의 최대값
data.max(axis=1) # data 각 행 단위의 최대값

# 수식으로 정규화 나타내기

# 열을 정규화시켜야 하는 하나의 데이터로 생각 
# 분모(최대값-최소값)
data.max(axis=0) - data.min(axis=0)
# 각 데이터 값-최소값=(분자) / 최대값 - 최소값(=분모)
data_mm = (data-data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))
data_mm

==> 수식으로 나타낸 정규화 과정
```

* 함수사용: MinMaxScaler

```python
# MinMaxScaler 함수 사용 정규화
mms=MinMaxScaler()
data_mms = mms.fit_transform(data)
data_mms 
```

* 함수사용: minmax_scale

```python
from sklearn.preprocessing import minmax_scale # minmax_scale 함수 

# minmax_scale 함수 이용 정규화 가능
datamm=minmax_scale(data,axis=0)
datamm
```





#### Binarizer (이항변수화)

**연속형 변수를 이항변수화**

: **연속형 데이터**를 기준을 두고 **0또는 1로 치환하는 것**으로 0 아니면 1 두가지의 값으로만 데이터를 변환.

> 연속형 변수를 기준이하 -> 0, 기준초과 -> 1

- 확률변수 x가 이항분포를 따른다고 했을때, 0 또는 1 값을 갖는 이항변수화가 필요

- 베르누이시행
  
    :실험을 반복해서 계속 실행하는 경우, 결과는 성공(1) 또는 실패(0) 2가지만 존재
    
    - 성공확률이 p인 베르누이시행을 n번 수행, 
    - 성공하는 횟수를 x라고 하면, 확률변수 x는 모수 n과 p인 이항분포를 따른다.

> 주사위 : 3이 나오면 성공, 아니면 실패
>
> 3이 나올 확률(성공 확률): 1/6	실패확률: 5/6

- A, B, C, D 네 사람이 주사위를 던질 때 한번씩 숫자 3이 나오는 경우 계산하기

> A만 성공, B만 성공, C만 성공, D만 성공(4가지 경우)의 확률 구하기
>
> A만 성공할 확률: 1/6 * 5/6 * 5/6 *5/6 = 0.09
>
> B만 성공할 확률: 1/6 * 5/6 * 5/6 *5/6 = 0.09
>
> C만 성공할 확률: 1/6 * 5/6 * 5/6 *5/6 = 0.09 
>
> D만 성공할 확률: 1/6 * 5/6 * 5/6 *5/6 = 0.09
>
> > 0.09 * 4 = 0.36(=최종 성공확률)
> >
> > 성공확률(p) * 시나리오수(n)



##### Binarizer로 이항변수화

* Binarizer()로 객체 만들고 fit()으로 data 넘겨줘야 함

* Binarizer(copy=True, threshold=0.0): default
  * thershold: 기준이 되는 임계값
    * threshold=0.0: 0.0값을 기준으로 1과 0으로 나누겠다는 의미
    * 기준이되는 값 thershold를 직접 지정해서 정할 수 있다.

```python
from sklearn.preprocessing import Binarizer # import 필요

data=np.array([[10,-10,2],[5,0,6],[0,7,4]])

bina=Binarizer().fit(data) # Binarizer()로 객체 만들어줘야 함
# ==> Binarizer(copy=True, threshold=0.0) : default 
bina.transform(data)
# ==> 0포함 음수는 0으로 양수는 1로 변환

# thershold 값 지정해주기
bina=Binarizer(threshold=5).fit(data) # 5이하(0), 5초과(1)로
bina.transform(data)
# ==> # 5이하는 0으로 5초과는 1로 변환
```



##### binarize로 이항변수화

* Binarizer()와 달리 객체 만들 필요 없이 바로 데이터 넣어주면 된다.
* copy 옵션
  * copy=True: default
  * copy=False이면, 원본데이터로 binarize하기 때문에 원본 변화 생김

```python
from sklearn.preprocessing import binarize # import 필요

data=np.array([[10,-10,2],[5,0,6],[0,7,4]])
binarize(data)

# thershold 값 지정해주기
binarize(data, threshold=5)

# copy 옵션
binarize(data, threshold=5, copy=True) # copy=True: default
binarize(data, threshold=5, copy=False) 
# copy=False: 원본데이터로 binarize > 원본 바뀜
```





#### OneHotEncoder (원핫인코더)

**범주형 변수(=카테고리로 나누어진 변수)를 이항변수화**

>  연령대: 20대 => 0, 30대 => 1, 40대 => 2
>
> 성별: 남성 =>0, 여성 =>1
>
> 학점: A:0, B:1, C:2, D:3, E:4
>
> 학점(범주형데이터)를 이항변수화(=추상화)
>
> > ​     학점
> >   0 1 2 3 4
> >   10000(A)
> >   01000(B)
> > ​       ...
> >   00001(F)

> 20대 여 B학점          30대 남 D학점
> 100 01 01000           010 10 00010

```python
from sklearn.preprocessing import OneHotEncoder # import 필요

#            [성별,연령,학점]
data=np.array([[0,0,0],
               [0,1,1],
               [0,2,2],
               [1,0,3],
               [1,1,4]])

# data 확인
data

# OneHotEncoder 객체 만들기
ohe=OneHotEncoder()
# fit(데이터): 주어진 데이터를 onehotencoding를 사용해 onehotencoder해주기
ohe.fit(data)
```

* OneHotEncoder 속성

```python
# 인덱스 확인
ohe.active_features_
==> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)
# 0 1: 성별/ 2 3 4: 연령대/ 5 6 7 8 9: 학점 나타내는 인덱스(자릿수)

# 범주 개수 확인
ohe.n_values_
==> array([2, 3, 5])
# 각 범주의 개수(성별:2, 연령:3, 학점:5)

# 범주 범위
ohe.feature_indices_
==> array([ 0,  2,  5, 10], dtype=int32)
# 범주에 대한 범위로 성별: 0이상 2미만, 연령대: 2이상 5미만, 학점: 5이상 10미만으로 나누어지는 것
```

##### 범주형 데이터 => 이항변수화

```python
# 여성 40대 D학점 data 만들기
data = np.array([[1,2,3]])

# data > 이항변수화
# toarray()로 출력해줘야 변환되어진 결과(=이항변수화) 확인 가능
ohe.transform(data).toarray() 
==> array([[0., 1., 0., 0., 1., 0., 0., 0., 1., 0.]])
             여성       40대            학점
```





#### 이산(형)화

: **연속형 변수를 2개 이상의 범주를 갖는 변수로 변환하는 작업**

(<==> 이항변수화: 0 또는 1 값을 갖는 변수를 만드는 것)

* 범주(=category)
  ex) 점수: 75(연속형 변수) => A/B/C/D/F(5개 범주갖는 범주형 데이터)

* **np.digitize**함수, **np.where**함수: 연속형 변수 => 이산화(=이산화작업)하는데 사용

- 이산(형)화 언제 사용하는가?
    - 구간별로 요약통계 구할 때 이산(형)화 많이 사용
    - 구간에 대한 평균값으로 범주간 평균 차이 구할 때
    - 독립성 검정
    - indexing: 인덱스 부여할 때

> seed: 난수 동일하게 나오도록 해줌
>
> seed로 동일한 난수 데이터 추출하는 이유
> 예측모델: 과거 데이터 기반으로 만들어지는 것으로 예측모델을 만들고자할 때,과거부터 현재까지의 데이터 변화 속 패턴 발견해야하는 것. 
>
> 패턴을 모델이라고 생각하면 된다. 패턴을 찾고자 하는 것
>
> 보통 과거의 데이터70%(=모델만들기용)를 가지고 패턴 찾아 모델을 만든 후,
> 모델이 제대로 동작하는지 확인하기 위해 남은 과거 데이터 30%를 가지고 테스트한다.
>
> > 데이터가 100개라면 100개 중 70개를 어떻게 골라내야할까? 
> >
> > 순서에 의해서 고르기보단 골고루 뽑아내는 것이 좋다. 순서대로 뽑으면 이미 패턴가지고 있을 수도 있기 때문에.
> > 데이터 뒤죽박죽 섞어 뽑을 때 주로 random 함수를 사용한다.
> >
> > 모델 만들고 테스트할 때 모델 만들 때 사용한 데이터가 고정되어있어야한다. 그래야 알고리즘 수정 후 모델 만들 때 이전의 데이터와 동일해야 성능이 좋아졌는지 직접적으로 비교할 수 있다.
> >
> > > np.random.seed(): 42,777,1004 주로 사용해서 데이터 추출 



* 카페에서 실습용코드 다운 > copy

##### np.digitize()로 이산화

```python
df = pd.DataFrame({'C1': np.random.randn(20),
   'C2': ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a','b', 'b',             'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']})

"""
df의 c1열 20개 data구성
> c1열에 저장된 난수값 이산화 > 10개의 bin으로 균등하게 나누기
"""
# 구간 만들어주기
df['C1'].min() # -1.322
df.C1.max() # 1.647
# linspace: 지정한 구간으로 균등하게 나눠주기
bins = np.linspace(df['C1'].min(),df.C1.max(), 10) # 10구간으로
==> 최소값 처음, 최대값 끝을 기준으로 균등하게 10구간으로 나눠진 결과 출력

# C1의 데이터를 bins로 수치화 시켜주면 이산화 되는 것 #
df.C1
df['C1_bin'] = np.digitize(df['C1'], bins)
df
==> 이산화 데이터로 만들어진 C1_bin이 df에 추가됨
==> 이산화 데이터(C1_bin)을 0,1로만 나눠주는 이항변수화 작업도 가능해짐
```



* 이산화 데이터로 만들어준 C1_bin 속성 확인

```python
df.C1_bin
# groupby: C1_bin 그룹화해서 같은 범주끼리 그룹화해주기
df.groupby('C1_bin') 

# 각 범주에 속한 데이터 건수 조사 가능: 그룹에 속한 데이터 몇개냐
df.groupby('C1_bin')['C1'].size() 

# 그룹화했을 때 그룹별 C1의 평균
df.groupby('C1_bin')['C1'].mean() 

# 그룹화했을 때 그룹별 C1의 표준편차
df.groupby('C1_bin')['C1'].std() 
==>그룹에 데이터가 1개인 범주는 NaN값 나옴

 # C1_bin 그룹화에 대해 C2를 참조
df.groupby('C1_bin')['C2'].value_counts()

# 그룹화(10개의 그룹)한 후 특정 그룹만 추출(참조) 가능 #
df[df['C1_bin']==4]
==> C1_bin인 부분만 추출되는 것 확인 가능
```

![이산화결과](images/이산화결과.png)

###### 이산화한 데이터 OneHotEncoding하기

* **get_dummies()** 사용해서 이산화한 데이터 원핫인코딩하기
  * pd.get_dummies():가변수 생성할 때 사용하는 함수

```python
# pd.get_dummies():가변수 생성할 때 사용하는 함수
pd.get_dummies(df['C1_bin'])
"""
모델 = x1*가중치+x2*가중치+...+x10*가중치+바이어스
"""
# 접두어 주는 옵션 사용
pd.get_dummies(df['C1_bin'], prefix='C1')
```

![이산화_원핫인코더](images/이산화_원핫인코더.png)



##### np.where()로 이산화

* np.where(조건, 참, 거짓)

```python
# 이산(형)화 작업: np.where() 사용

# np.where로 2개의 범주로 나눠주기
df['C1'] >= df.C1.mean()
df['h_l'] = np.where(df['C1'] >= df.C1.mean(),"high",'low')

# 범주별로 요약통계 > 그룹화 통해 그룹별 통계치 구할 수 있음
df.groupby('h_l')['C1'].size()
df.groupby('h_l')['C1'].mean()
df.groupby('h_l')['C1'].std()


# percentile 사용해서 사분위수(25%) 구하기
Q1 = np.percentile(df['C1'],25)
Q3 = np.percentile(df['C1'],75)
"""
연습문제
C1열 값 < Q1 then df['h_m_l'] =low
C1열 값 > Q1 and <=Q3 then df['h_m_l'] = medium
else then df['h_m_l'] = high
"""
df['h_m_l'] = np.where(df['C1']<Q1, 'low', np.where(df['C1'] >= Q3, 'high', 'medium'))
df
```

![np.where_이산화](images/np.where_이산화.png)





#### 데이터 재구조화

- 피벗테이블(pivot, pivot_table)
    - 가장 많이 쓰이는 것이 피벗 테이블, 그 다음에 스택 함수가 주로 쓰임
- 스택(stack) 함수: 데이터를 원하는 모습으로 쌓고자 할 때
    - 언스택(unstack): 쌓아놓은 데이터를 원래대로
- 멜트(melt): 데이터를 녹인 다음에 다시 구조 바꿔주는 것
- wide_to_long
- crosstab



##### 피벗테이블

> 원하는 열만 추출하여 원하는 구조로 변경 가능
>
> data의 특정열을 데이터로(특정열의 열요소를 열로, 특정열을 행으로) 자료 구조 만들고 싶을 때 사용

```python
data = pd.DataFrame({'cust_id': ['c1', 'c1', 'c1', 'c2', 'c2', 'c2', 'c3', 'c3', 'c3'], 'prod_cd': ['p1', 'p2', 'p3', 'p1', 'p2', 'p3', 'p1', 'p2', 'p3'],'grade' : ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B'],'pch_amt': [30, 10, 0, 40, 15, 30, 0, 0, 10]})

"""
- 위의 데이터를 밑의 자료구조로 바꾸고자 할 때 pivot 사용
    p1  p2  p3
cid
c1  30  10  0
c2  40  15  30
c3  ...
"""
```

###### pivot () 사용

> pivot은 dataframe에 있는 함수
>
> index에 행으로 보내고 싶은 해당 열, columns에 열로 만들고 싶은 열, values에 데이터로 만들고 싶은 열 지정해준다.
>
> index, columns, values 지정해주는 것 기억해야!

```python
# pivot 함수 사용

data.pivot(index='cust_id', columns='prod_cd', values='pch_amt')
```

![피벗함수사용](images/피벗함수사용.png)

###### pivot_table() 사용

> pivot_table은 pandas에 있는 함수
>
> **pivot()과 pivot_table()은 같은 결과 출력, pivot_table()로 가능한 상황 더 많아 pivot_table() 사용하는 것이 편리 **
> pivot_table: 집계함수 제공해서 분석작업 쉽게 만들어줌

```python
# pivot_table()
pd.pivot_table(data,index='cust_id', columns='prod_cd', values='pch_amt')
```

* pivot_table로만 가능, pivot으로 구현 불가능한 경우
  * index 계층화하는 경우

```python
data.pivot(index=['cust_id','grade'],columns='prod_cd', values='pch_amt') 
==> 에러

# pivot_table 가능
pd.pivot_table(data,index=['cust_id','grade'], columns='prod_cd', values='pch_amt')

# pivot_table만 가능한 경우: 열(column)도 동일
pd.pivot_table(data,index='cust_id', columns=['grade','prod_cd'], values='pch_amt')
```

![피벗_계층화](images/피벗_계층화.png)

* pivot_table: 집계함수 

```python
# pivot_table: 집계함수 제공
pd.pivot_table(data, index='grade', columns='prod_cd', values='pch_amt')
# ==> 평균값 적용되어서 결과 출력됨: default가 평균 == aggfunc=np.mean
pd.pivot_table(data, index='grade', columns='prod_cd', values='pch_amt', aggfunc=np.mean)

# sum값 출력: aggfunc=np.sum
pd.pivot_table(data, index='grade', columns='prod_cd', values='pch_amt', aggfunc=np.sum)

"""
pivot 함수로는 불가능 => pivot_table 사용이 편리
pivot error: pivot() got an unexpected keyword argument 'aggfunc'
data.pivot(index='grade', columns='prod_cd', values='pch_amt', aggfunc=np.sum)
grade 중복 많아 error: Index contains duplicate entries, cannot reshape
data.pivot(index='grade', columns='prod_cd', values='pch_amt')
"""
```



##### stack(스택)함수

* 데이터 변형 stack()), unstack() 사용
  * stack(): 위에서 아래로 쌓는 것
  * unstack(): 쌓여있는 것을 옆으로 놓는 것(왼=>오)

```python
# MultiIndex 지정
mul_index = pd.MultiIndex.from_tuples([('cust_1', '2020'), ('cust_1', '2021'),('cust_2', '2020'), ('cust_2', '2021')])  

# index를 mul_index로 지정해준 다중계층 데이터 생성
data = pd.DataFrame(data=np.arange(16).reshape(4, 4),index=mul_index,columns=['prd_1', 'prd_2', 'prd_3', 'prd_4'],dtype='int')
data
```

![mul_index](images/mul_index.png)

* stack() 사용

```python
# stack()로 데이터 위에서 아래로 쌓아보기
datastd=data.stack()
datastd
==> column index로 있던 것들이 행 계층으로 오게 됨. 행인덱스 계층에 추가됨

# index 확인
datastd.index

# data 참조
# cust_2  2020  prd_1     8
#               prd_2     9
# 대분류부터 소분류까지 하나씩 들어가면서 참조
datastd['cust_2']['2020'][['prd_1','prd_2']] 
# data2개 참조 지정: 대괄호[[]] 두개 써줘야 함


# data > prd_4 11, 15자리에 None 넣기
data.ix['cust_2','prd_4'] = np.nan
data

data.stack() # 위에서 만들었던 nan값이 제거되어서 출력됨.
data.stack(dropna=False) # nan 출력하기위한 옵션
```



##### melt(멜트) 함수

```python
data = pd.DataFrame({'cust_ID' : ['C_001', 'C_001', 'C_002', 'C_002'],
              'prd_CD' : ['P_001', 'P_002', 'P_001', 'P_002'],
              'pch_cnt' : [1, 2, 3, 4],'pch_amt' : [100, 200, 300, 400]})
```

![melt데이터](images/melt데이터-1578993164244.png)

```python
# melt() 사용
pd.melt(data, id_vars=['cust_ID','prd_CD'])

# var,value name 지정 가능
pd.melt(data, id_vars=['cust_ID','prd_CD'], var_name='CD',value_name='VAL')
```

![melt_후](images/melt_후.png)

* 연습 1

```python
# 연습문제, 강사님 data 파일에서 pew.csv가져오기

pew = pd.read_csv('data/pew.csv')
pew.head()

# iloc 사용해서 40~50k 데이터까지만 출력
pew.iloc[:,0:6]

# melt() 사용해서 religion	variable value만 출력하기
pd.melt(pew, id_vars='religion', var_name='income', value_name='count')
```

* 연습 2

```python
# 연습문제, 강사님 data 파일에서 billboard.csv가져오기

billboard=pd.read_csv("data/billboard.csv")
billboard

pd.melt(billboard, id_vars=['year','artist','track','time','date.entered	'], var_name='week',value_name='rating')
# 위의 코드 iloc 사용해서도 구현 가능
pd.melt(billboard, id_vars=billboard.iloc[:,0:5], var_name='week',value_name='rating')
```





#### 데이터 개수 확인

##### count()

```python
# data 개수 세기

# series에서
s=pd.Series(range(10))
s[3]=np.nan # NaN값 넣어주기
s.count() # 9: NaN인 누락값은 제외


# dataframe에서
df=pd.DataFrame(np.random.randint(5,size=(4,4)),dtype=float)
# 2행 3열 data를 nan으로 만들기
df.iloc[2,3]=np.nan
df.count() 
==> 4,4,4,3(각 열 데이터 개수)로 결측값 포함 x
```

* titanic data

```python
import seaborn as sns
# seaborn 패키지에서 titanic 데이터 제공해줌
titanic = sns.load_dataset("titanic") 
titanic.head()

titanic.count() 
==> 각 열별 데이터 건수 나옴: 결측값 얼마나 있는지 파악 가능 
==> series에 대해서 count가 적용된 것
```



##### value_counts()

: 종류별로 데이터 개수를 count + 내림차순(sort)로 정렬

```python
# value_counts()

# series에서
np.random.seed(777)
s2 = pd.Series(np.random.randint(6, size=100))
s2.count()
s2.value_counts() # 종류별로 데이터 개수를 count

# sort_index(): index 기준으로 데이터 정렬
s2.value_counts().sort_index()


# dataframe에서
df.value_counts() 
==> Error:'DataFrame' object has no attribute 'value_counts'
# 데이터프레임에서는 value_counts() 없어 열 별로 해줘야 함(=series 단위로)
df[0].value_counts()
```



##### sort_values()

```python
s.sort_values() # 값을 기준으로 오름차순 정렬(NaN은 맨 마지막에 들어감)
s.sort_values(ascending=False) 
#ascending=False 옵션으로 주면 내림차순 정렬로 출력


# 데이터프레임에서 sort_values() 쓰기
df.sort_values(by=2) # by 뒤에 오는 index 기준으로 정렬
df.sort_values(by=[1,2])
```



* dataframe 

```python
df = pd.DataFrame(np.random.randint(10,size=(4,8)))
df

# df에 열인덱스 추가하기
df['Mysum']=df.sum(axis=1)
df

# df에 total이란 행인덱스 추가하기
df.loc['total', : ] = df.sum()
```





#### 실수데이터 => 범주화

* cut, qcut() 사용
  - cut(): 실수값의 경계 지정
  - qcut(): 똑같은 구간으로 나눔

```python
ages=[0,2,10,21,23,37,31,61,20,42,32,100]
bins=[1,15,25,35,60,99]
labels=['미성년자','청년','중년','장년','노년']

cats=pd.cut(ages, bins, labels=labels)
cats
# 0,100은 bins 범위에 해당 x => NaN으로

# data 확인

# categories 확인
cats.categories
==> Index(['미성년자', '청년', '중년', '장년', '노년'], dtype='object')

# codes 확인
cats.codes
==> array([-1,  0,  0,  1,  1,  3,  2,  4,  1,  3,  2, -1], dtype=int8)


# 데이터프레임으로 만들어주기
df = pd.DataFrame(ages, columns=['ages'])
df['age_cat'] = pd.cut(ages,bins,labels=labels)
df
```

```python
data = np.random.randn(100)
data

ats=pd.qcut(data,4, labels=["Q1","Q2","Q3","Q4"])
cats

pd.value_counts(cats)
```





